{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Recurrent Q-Network \n",
    "This notebook provides an example implementation of a Deep Recurrent Q-Network which can solve Partially Observable Markov Decision Processes. To learn more about DRQNs, see my blog post on them here: https://medium.com/p/68463e9aeefc .\n",
    "\n",
    "For more reinforcment learning tutorials, as well as the additional required `gridworld.py` and `helper.py` see:\n",
    "https://github.com/awjuliani/DeepRL-Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/junhyeok/InteroceptiveAI\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "# !pip install helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.misc\n",
    "import os\n",
    "import csv\n",
    "import itertools\n",
    "import tensorflow.contrib.slim as slim\n",
    "%matplotlib inline\n",
    "\n",
    "from helper import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the game environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version:\n",
      "3.6.12 |Anaconda, Inc.| (default, Sep  8 2020, 23:10:56) \n",
      "[GCC 7.3.0]\n"
     ]
    }
   ],
   "source": [
    "# from COGNIADemo.COGNIADemoEnv import Environment\n",
    "env_name = \"./env_linux/H\"  # Name of the Unity environment binary to launch\n",
    "train_mode = False  # Whether to run the environment in training or inference mode\n",
    "import sys\n",
    "\n",
    "from mlagents.envs import UnityEnvironment\n",
    "\n",
    "print(\"Python version:\")\n",
    "print(sys.version)\n",
    "\n",
    "# check Python version\n",
    "if (sys.version_info[0] < 3):\n",
    "    raise Exception(\"ERROR: ML-Agents Toolkit (v0.3 onwards) requires Python 3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the network itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class Qnetwork:\n",
    "    def __init__(self, h_size, rnn_cell, myScope):\n",
    "        # The network recieves a frame from the game, flattened into an array.\n",
    "        # It then resizes it and processes it through four convolutional layers.\n",
    "        self.scalarInput = tf.placeholder(\n",
    "            shape=[None, 21168], dtype=tf.float32\n",
    "        )  # 3*84*84\n",
    "        self.imageIn = tf.reshape(self.scalarInput, shape=[-1, 84, 84, 3])\n",
    "        self.conv1 = slim.convolution2d(\n",
    "            inputs=self.imageIn,\n",
    "            num_outputs=32,\n",
    "            kernel_size=[8, 8],\n",
    "            stride=[4, 4],\n",
    "            padding=\"VALID\",\n",
    "            biases_initializer=None,\n",
    "            scope=myScope + \"_conv1\",\n",
    "        )\n",
    "        self.conv2 = slim.convolution2d(\n",
    "            inputs=self.conv1,\n",
    "            num_outputs=64,\n",
    "            kernel_size=[4, 4],\n",
    "            stride=[2, 2],\n",
    "            padding=\"VALID\",\n",
    "            biases_initializer=None,\n",
    "            scope=myScope + \"_conv2\",\n",
    "        )\n",
    "        self.conv3 = slim.convolution2d(\n",
    "            inputs=self.conv2,\n",
    "            num_outputs=64,\n",
    "            kernel_size=[3, 3],\n",
    "            stride=[1, 1],\n",
    "            padding=\"VALID\",\n",
    "            biases_initializer=None,\n",
    "            scope=myScope + \"_conv3\",\n",
    "        )\n",
    "        self.conv4 = slim.convolution2d(\n",
    "            inputs=self.conv3,\n",
    "            num_outputs=h_size,\n",
    "            kernel_size=[7, 7],\n",
    "            stride=[1, 1],\n",
    "            padding=\"VALID\",\n",
    "            biases_initializer=None,\n",
    "            scope=myScope + \"_conv4\",\n",
    "        )\n",
    "\n",
    "        self.trainLength = tf.placeholder(dtype=tf.int32)\n",
    "        # We take the output from the final convolutional layer and send it to a recurrent layer.\n",
    "        # The input must be reshaped into [batch x trace x units] for rnn processing,\n",
    "        # and then returned to [batch x units] when sent through the upper levles.\n",
    "        self.batch_size = tf.placeholder(dtype=tf.int32, shape=[])\n",
    "        self.convFlat = tf.reshape(\n",
    "            slim.flatten(self.conv4), [self.batch_size, self.trainLength, h_size]\n",
    "        )\n",
    "        self.state_in = rnn_cell.zero_state(self.batch_size, tf.float32)\n",
    "        self.rnn, self.rnn_state = tf.nn.dynamic_rnn(\n",
    "            inputs=self.convFlat,\n",
    "            cell=rnn_cell,\n",
    "            dtype=tf.float32,\n",
    "            initial_state=self.state_in,\n",
    "            scope=myScope + \"_rnn\",\n",
    "        )\n",
    "        self.rnn = tf.reshape(self.rnn, shape=[-1, h_size])\n",
    "        # The output from the recurrent player is then split into separate Value and Advantage streams\n",
    "        self.streamA, self.streamV = tf.split(self.rnn, 2, 1)\n",
    "        self.AW = tf.Variable(tf.random_normal([h_size // 2, 4]))\n",
    "        self.VW = tf.Variable(tf.random_normal([h_size // 2, 1]))\n",
    "        self.Advantage = tf.matmul(self.streamA, self.AW)\n",
    "        self.Value = tf.matmul(self.streamV, self.VW)\n",
    "\n",
    "        self.salience = tf.gradients(self.Advantage, self.imageIn)\n",
    "        # Then combine them together to get our final Q-values.\n",
    "        self.Qout = self.Value + tf.subtract(\n",
    "            self.Advantage, tf.reduce_mean(self.Advantage, axis=1, keep_dims=True)\n",
    "        )\n",
    "        self.predict = tf.argmax(self.Qout, 1)\n",
    "\n",
    "        # Below we obtain the loss by taking the sum of squares difference between the target and prediction Q values.\n",
    "        self.targetQ = tf.placeholder(shape=[None], dtype=tf.float32)\n",
    "        self.actions = tf.placeholder(shape=[None], dtype=tf.int32)\n",
    "        self.actions_onehot = tf.one_hot(self.actions, 4, dtype=tf.float32)\n",
    "\n",
    "        self.Q = tf.reduce_sum(tf.multiply(self.Qout, self.actions_onehot), axis=1)\n",
    "\n",
    "        self.td_error = tf.square(self.targetQ - self.Q)\n",
    "\n",
    "        # In order to only propogate accurate gradients through the network, we will mask the first\n",
    "        # half of the losses for each trace as per Lample & Chatlot 2016\n",
    "        self.maskA = tf.zeros([self.batch_size, self.trainLength // 2])\n",
    "        self.maskB = tf.ones([self.batch_size, self.trainLength // 2])\n",
    "        self.mask = tf.concat([self.maskA, self.maskB], 1)\n",
    "        self.mask = tf.reshape(self.mask, [-1])\n",
    "        self.loss = tf.reduce_mean(self.td_error * self.mask)\n",
    "\n",
    "        self.trainer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "        self.updateModel = self.trainer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experience Replay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These classes allow us to store experies and sample then randomly to train the network.\n",
    "Episode buffer stores experiences for each individal episode.\n",
    "Experience buffer stores entire episodes of experience, and sample() allows us to get training batches needed from the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class experience_buffer():\n",
    "    def __init__(self, buffer_size = 1000):\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "    \n",
    "    def add(self,experience):\n",
    "        if len(self.buffer) + 1 >= self.buffer_size:\n",
    "            self.buffer[0:(1+len(self.buffer))-self.buffer_size] = []\n",
    "        self.buffer.append(experience)\n",
    "            \n",
    "    def sample(self,batch_size,trace_length):\n",
    "        sampled_episodes = random.sample(self.buffer,batch_size)\n",
    "        sampledTraces = []\n",
    "        for episode in sampled_episodes:\n",
    "            point = np.random.randint(0,len(episode)+1-trace_length)\n",
    "            sampledTraces.append(episode[point:point+trace_length])\n",
    "        sampledTraces = np.array(sampledTraces)\n",
    "        return np.reshape(sampledTraces,[batch_size*trace_length,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the training parameters\n",
    "batch_size = 4  # How many experience traces to use for each training step.\n",
    "trace_length = 8  # How long each experience trace will be when training\n",
    "update_freq = 5  # How often to perform a training step.\n",
    "y = 0.99  # Discount factor on the target Q-values\n",
    "startE = 1  # Starting chance of random action\n",
    "endE = 0.1  # Final chance of random action\n",
    "anneling_steps = 10000  # How many steps of training to reduce startE to endE.\n",
    "num_episodes = 10000  # How many episodes of game environment to train network with.\n",
    "pre_train_steps = 10000  # How many steps of random actions before training begins.\n",
    "load_model = False  # Whether to load a saved model.\n",
    "path = \"./saved_models\"  # The path to save our model to.\n",
    "h_size = 512  # The size of the final convolutional layer before splitting it into Advantage and Value streams.\n",
    "max_epLength = 50  # The max allowed length of our episode.\n",
    "time_per_step = 1  # Length of each step used in gif creation\n",
    "summaryLength = 100  # Number of epidoes to periodically save for analysis\n",
    "tau = 0.001\n",
    "\n",
    "movementSize = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-1d901aacbee4>:73: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-1d901aacbee4>:73: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n##Write the first line of the master log-file for the Control Center\\nwith open('./Center/log.csv', 'w') as myfile:\\n    wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\\n    wr.writerow(['Episode','Length','Reward','IMG','LOG','SAL'])    \\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "# We define the cells for the primary and target q-networks\n",
    "cell = tf.contrib.rnn.BasicLSTMCell(num_units=h_size, state_is_tuple=True)\n",
    "cellT = tf.contrib.rnn.BasicLSTMCell(num_units=h_size, state_is_tuple=True)\n",
    "mainQN = Qnetwork(h_size, cell, \"main\")\n",
    "targetQN = Qnetwork(h_size, cellT, \"target\")\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=5)\n",
    "\n",
    "trainables = tf.trainable_variables()\n",
    "\n",
    "targetOps = updateTargetGraph(trainables, tau)\n",
    "\n",
    "myBuffer = experience_buffer()\n",
    "\n",
    "# Set the rate of random action decrease.\n",
    "e = startE\n",
    "stepDrop = (startE - endE) / anneling_steps\n",
    "\n",
    "# create lists to contain total rewards and steps per episode\n",
    "jList = []\n",
    "rList = []\n",
    "total_steps = 0\n",
    "\n",
    "# Make a path for our model to be saved in.\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "\"\"\"\n",
    "##Write the first line of the master log-file for the Control Center\n",
    "with open('./Center/log.csv', 'w') as myfile:\n",
    "    wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "    wr.writerow(['Episode','Length','Reward','IMG','LOG','SAL'])    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mlagents.envs:\n",
      "'HRL Academy' started successfully!\n",
      "Unity Academy name: HRL Academy\n",
      "        Number of Brains: 2\n",
      "        Number of Training Brains : 1\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: HRLLearningBrain\n",
      "        Number of Visual Observations (per agent): 1\n",
      "        Vector Observation space size (per agent): 2\n",
      "        Number of stacked Vector Observation: 4\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): [3, 3, 3, 3, 3]\n",
      "        Vector Action descriptions: Vertical, Left, Right, Jump, None\n",
      "Unity brain name: HRLPlayerBrain\n",
      "        Number of Visual Observations (per agent): 1\n",
      "        Vector Observation space size (per agent): 2\n",
      "        Number of stacked Vector Observation: 4\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): [3, 3, 3, 3, 3]\n",
      "        Vector Action descriptions: Vertical, Left, Right, Consume, None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent state looks like: \n",
      "[  0.   0.   0.   0.   0.   0. 100. 100.]\n",
      "Agent observations look like:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#HRLenv\n",
    "env = UnityEnvironment(file_name=env_name)\n",
    "   \n",
    "# Set the default brain to work with\n",
    "default_brain = env.brain_names[0]\n",
    "brain = env.brains[default_brain]\n",
    "\n",
    "# Reset the environment\n",
    "env_info = env.reset(train_mode=train_mode)[default_brain]\n",
    "\n",
    "# Examine the state space for the default brain\n",
    "print(\"Agent state looks like: \\n{}\".format(env_info.vector_observations[0]))\n",
    "\n",
    "# Examine the observation space for the default brain\n",
    "for image in np.array(env_info.visual_observations):\n",
    "    print(\"Agent observations look like:\")\n",
    "    if image.shape[3] == 3:\n",
    "        plt.imshow(image[0,:,:,:])\n",
    "    else:\n",
    "        plt.imshow(image[0,:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2, 2, 0]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Set Success\n",
      "5000 -0.04500000213738531 1\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './Center/log.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-335b0fb8f0f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    155\u001b[0m                 \u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m                 \u001b[0mmainQN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m                 \u001b[0mtime_per_step\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m             )\n\u001b[1;32m    159\u001b[0m     \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/model-\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".cptk\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/InteroceptiveAI/helper.py\u001b[0m in \u001b[0;36msaveToCenter\u001b[0;34m(i, rList, jList, bufferArray, summaryLength, h_size, sess, mainQN, time_per_step)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m#Record performance metrics and episode logs for the Control Center.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msaveToCenter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrList\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mjList\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbufferArray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msummaryLength\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmainQN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtime_per_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./Center/log.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmyfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mstate_display\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mimagesS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './Center/log.csv'"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    if load_model == True:\n",
    "        print(\"Loading Model...\")\n",
    "        ckpt = tf.train.get_checkpoint_state(path)\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    sess.run(init)\n",
    "\n",
    "    # Set the target network to be equal to the primary network.\n",
    "    updateTarget(targetOps, sess)\n",
    "    env_info = env.reset(train_mode=train_mode)[default_brain]\n",
    "    for i in range(num_episodes):\n",
    "        episodeBuffer = []\n",
    "\n",
    "        # Reset environment and get first new observation\n",
    "        action_size = brain.vector_action_space_size\n",
    "        action = np.column_stack([np.random.randint(0, action_size[i], size=(len(env_info.agents))) for i in range(len(action_size))])\n",
    "        \n",
    "        env_info = env.step(action)[default_brain]\n",
    "        r = env_info.rewards[0]\n",
    "        d = env_info.local_done[0]\n",
    "        image = np.array(env_info.visual_observations)[0,0,:,:,:]\n",
    "        \n",
    "        sP = np.transpose(image, (1, 2, 0))\n",
    "        s = processState(sP)\n",
    "        d = False\n",
    "        rAll = 0\n",
    "        j = 0\n",
    "\n",
    "        # Reset the recurrent layer's hidden state\n",
    "        state = (\n",
    "            np.zeros([1, h_size]),\n",
    "            np.zeros([1, h_size]),\n",
    "        )\n",
    "        # The Q-Network\n",
    "        while j < max_epLength:\n",
    "            j += 1\n",
    "            # Choose an action by greedily (with e chance of random action) from the Q-network\n",
    "            if np.random.rand(1) < e or total_steps < pre_train_steps:\n",
    "                state1 = sess.run(\n",
    "                    mainQN.rnn_state,\n",
    "                    feed_dict={\n",
    "                        mainQN.scalarInput: [s / 255.0],\n",
    "                        mainQN.trainLength: 1,\n",
    "                        mainQN.state_in: state,\n",
    "                        mainQN.batch_size: 1,\n",
    "                    },\n",
    "                )\n",
    "#                 a = np.random.randint(0, 4)\n",
    "                a = np.column_stack([np.random.randint(0, action_size[i], size=(len(env_info.agents))) for i in range(len(action_size))])\n",
    "            else:\n",
    "                a, state1 = sess.run(\n",
    "                    [mainQN.predict, mainQN.rnn_state],\n",
    "                    feed_dict={\n",
    "                        mainQN.scalarInput: [s / 255.0],\n",
    "                        mainQN.trainLength: 1,\n",
    "                        mainQN.state_in: state,\n",
    "                        mainQN.batch_size: 1,\n",
    "                    },\n",
    "                )\n",
    "                a = a[0]\n",
    "\n",
    "#             # COGNIA\n",
    "#             if a == 0:\n",
    "#                 a = [[-1 * movementSize, 0]]  # backward\n",
    "#             elif a == 1:\n",
    "#                 a = [[1 * movementSize, 0]]  # foward\n",
    "#             elif a == 2:\n",
    "#                 a = [[0, -1]]  # right\n",
    "#             elif a == 3:\n",
    "#                 a = [[0, 1]]  # left\n",
    "            env_info = env.step(action)[default_brain]\n",
    "            r = env_info.rewards[0]\n",
    "            d = env_info.local_done[0]\n",
    "            image = np.array(env_info.visual_observations)[0,0,:,:,:]\n",
    "#             s1P, r, d = env.step(a)\n",
    "            s1P = np.transpose(image, (1, 2, 0))\n",
    "            s1 = processState(s1P)\n",
    "            total_steps += 1\n",
    "            episodeBuffer.append(np.reshape(np.array([s, a, r, s1, d]), [1, 5]))\n",
    "            if total_steps > pre_train_steps:\n",
    "                if e > endE:\n",
    "                    e -= stepDrop\n",
    "\n",
    "                if total_steps % (update_freq) == 0:\n",
    "                    updateTarget(targetOps, sess)\n",
    "                    # Reset the recurrent layer's hidden state\n",
    "                    state_train = (\n",
    "                        np.zeros([batch_size, h_size]),\n",
    "                        np.zeros([batch_size, h_size]),\n",
    "                    )\n",
    "\n",
    "                    # Get a random batch of experiences.\n",
    "                    trainBatch = myBuffer.sample(batch_size, trace_length)\n",
    "                    # Below we perform the Double-DQN update to the target Q-values\n",
    "                    Q1 = sess.run(\n",
    "                        mainQN.predict,\n",
    "                        feed_dict={\n",
    "                            mainQN.scalarInput: np.vstack(trainBatch[:, 3] / 255.0),\n",
    "                            mainQN.trainLength: trace_length,\n",
    "                            mainQN.state_in: state_train,\n",
    "                            mainQN.batch_size: batch_size,\n",
    "                        },\n",
    "                    )\n",
    "                    Q2 = sess.run(\n",
    "                        targetQN.Qout,\n",
    "                        feed_dict={\n",
    "                            targetQN.scalarInput: np.vstack(trainBatch[:, 3] / 255.0),\n",
    "                            targetQN.trainLength: trace_length,\n",
    "                            targetQN.state_in: state_train,\n",
    "                            targetQN.batch_size: batch_size,\n",
    "                        },\n",
    "                    )\n",
    "                    end_multiplier = -(trainBatch[:, 4] - 1)\n",
    "                    doubleQ = Q2[range(batch_size * trace_length), Q1]\n",
    "                    targetQ = trainBatch[:, 2] + (y * doubleQ * end_multiplier)\n",
    "                    # Update the network with our target values.\n",
    "                    sess.run(\n",
    "                        mainQN.updateModel,\n",
    "                        feed_dict={\n",
    "                            mainQN.scalarInput: np.vstack(trainBatch[:, 0] / 255.0),\n",
    "                            mainQN.targetQ: targetQ,\n",
    "                            mainQN.actions: trainBatch[:, 1],\n",
    "                            mainQN.trainLength: trace_length,\n",
    "                            mainQN.state_in: state_train,\n",
    "                            mainQN.batch_size: batch_size,\n",
    "                        },\n",
    "                    )\n",
    "            rAll += r\n",
    "            s = s1\n",
    "            sP = s1P\n",
    "            state = state1\n",
    "            if d == True:\n",
    "                break\n",
    "\n",
    "        # Add the episode to the experience buffer\n",
    "        bufferArray = np.array(episodeBuffer)\n",
    "        episodeBuffer = list(zip(bufferArray))\n",
    "        myBuffer.add(episodeBuffer)\n",
    "        jList.append(j)\n",
    "        rList.append(rAll)\n",
    "\n",
    "        # Periodically save the model.\n",
    "        if i % 1000 == 0 and i != 0:\n",
    "            saver.save(sess, path + \"/model-\" + str(i) + \".cptk\")\n",
    "            print(\"Saved Model\")\n",
    "        if len(rList) % summaryLength == 0 and len(rList) != 0:\n",
    "            print(total_steps, np.mean(rList[-summaryLength:]), e)\n",
    "            saveToCenter(\n",
    "                i,\n",
    "                rList,\n",
    "                jList,\n",
    "                np.reshape(np.array(episodeBuffer), [len(episodeBuffer), 5]),\n",
    "                summaryLength,\n",
    "                h_size,\n",
    "                sess,\n",
    "                mainQN,\n",
    "                time_per_step,\n",
    "            )\n",
    "    saver.save(sess, path + \"/model-\" + str(i) + \".cptk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = 0.01  # The chance of chosing a random action\n",
    "num_episodes = 10000  # How many episodes of game environment to train network with.\n",
    "load_model = True  # Whether to load a saved model.\n",
    "path = \"./drqn\"  # The path to save/load our model to/from.\n",
    "h_size = 512  # The size of the final convolutional layer before splitting it into Advantage and Value streams.\n",
    "h_size = 512  # The size of the final convolutional layer before splitting it into Advantage and Value streams.\n",
    "max_epLength = 50  # The max allowed length of our episode.\n",
    "time_per_step = 1  # Length of each step used in gif creation\n",
    "summaryLength = 100  # Number of epidoes to periodically save for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "cell = tf.contrib.rnn.BasicLSTMCell(num_units=h_size, state_is_tuple=True)\n",
    "cellT = tf.contrib.rnn.BasicLSTMCell(num_units=h_size, state_is_tuple=True)\n",
    "mainQN = Qnetwork(h_size, cell, \"main\")\n",
    "targetQN = Qnetwork(h_size, cellT, \"target\")\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=2)\n",
    "\n",
    "# create lists to contain total rewards and steps per episode\n",
    "jList = []\n",
    "rList = []\n",
    "total_steps = 0\n",
    "\n",
    "# Make a path for our model to be saved in.\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "\"\"\"\n",
    "##Write the first line of the master log-file for the Control Center\n",
    "with open('./Center/log.csv', 'w') as myfile:\n",
    "    wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "    wr.writerow(['Episode','Length','Reward','IMG','LOG','SAL'])    \n",
    "    \n",
    "    #wr = csv.writer(open('./Center/log.csv', 'a'), quoting=csv.QUOTE_ALL)\n",
    "\"\"\"\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    if load_model == True:\n",
    "        print(\"Loading Model...\")\n",
    "        ckpt = tf.train.get_checkpoint_state(path)\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "        sess.run(init)\n",
    "\n",
    "    for i in range(num_episodes):\n",
    "        episodeBuffer = []\n",
    "        # Reset environment and get first new observation\n",
    "        sP = env.reset()\n",
    "        s = processState(sP)\n",
    "        d = False\n",
    "        rAll = 0\n",
    "        j = 0\n",
    "        state = (np.zeros([1, h_size]), np.zeros([1, h_size]))\n",
    "        # The Q-Network\n",
    "        # If the agent takes longer than 200 moves to reach either of the blocks, end the trial.\n",
    "        while j < max_epLength:\n",
    "            j += 1\n",
    "            # Choose an action by greedily (with e chance of random action) from the Q-network\n",
    "            if np.random.rand(1) < e:\n",
    "                state1 = sess.run(\n",
    "                    mainQN.rnn_state,\n",
    "                    feed_dict={\n",
    "                        mainQN.scalarInput: [s / 255.0],\n",
    "                        mainQN.trainLength: 1,\n",
    "                        mainQN.state_in: state,\n",
    "                        mainQN.batch_size: 1,\n",
    "                    },\n",
    "                )\n",
    "#                 a = np.random.randint(0, 4)\n",
    "                a = np.column_stack([np.random.randint(0, action_size[i], size=(len(env_info.agents))) for i in range(len(action_size))])\n",
    "            else:\n",
    "                a, state1 = sess.run(\n",
    "                    [mainQN.predict, mainQN.rnn_state],\n",
    "                    feed_dict={\n",
    "                        mainQN.scalarInput: [s / 255.0],\n",
    "                        mainQN.trainLength: 1,\n",
    "                        mainQN.state_in: state,\n",
    "                        mainQN.batch_size: 1,\n",
    "                    },\n",
    "                )\n",
    "                a = a[0]\n",
    "            s1P, r, d = env.step(a)\n",
    "            s1 = processState(s1P)\n",
    "            total_steps += 1\n",
    "            # Save the experience to our episode buffer.\n",
    "            episodeBuffer.append(np.reshape(np.array([s, a, r, s1, d]), [1, 5]))\n",
    "\n",
    "            rAll += r\n",
    "            s = s1\n",
    "            sP = s1P\n",
    "            state = state1\n",
    "            if d == True:\n",
    "                break\n",
    "\n",
    "        bufferArray = np.array(episodeBuffer)\n",
    "        jList.append(j)\n",
    "        rList.append(rAll)\n",
    "\n",
    "        # Periodically save the model.\n",
    "        if len(rList) % summaryLength == 0 and len(rList) != 0:\n",
    "            print(total_steps, np.mean(rList[-summaryLength:]), e)\n",
    "            saveToCenter(\n",
    "                i,\n",
    "                rList,\n",
    "                jList,\n",
    "                np.reshape(np.array(episodeBuffer), [len(episodeBuffer), 5]),\n",
    "                summaryLength,\n",
    "                h_size,\n",
    "                sess,\n",
    "                mainQN,\n",
    "                time_per_step,\n",
    "            )\n",
    "print(\"Percent of succesful episodes: \" + str(sum(rList) / num_episodes) + \"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HRL",
   "language": "python",
   "name": "hrl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
